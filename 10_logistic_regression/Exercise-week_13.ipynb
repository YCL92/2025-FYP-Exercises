{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef942ff",
   "metadata": {},
   "source": [
    "# Exercises for Week 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f650df3",
   "metadata": {},
   "source": [
    "#### Linear regression (naive vs. accelerated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211cf994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic data\n",
    "np.random.seed(0)\n",
    "n_samples = 1000\n",
    "X = np.linspace(0, 10, n_samples)\n",
    "true_slope = 3\n",
    "true_intercept = 2\n",
    "noise = np.random.randn(n_samples) * 2\n",
    "y = true_slope * X + true_intercept + noise\n",
    "\n",
    "# least squares using for loop\n",
    "def leastSquaresNaive(X, y):\n",
    "    # check slide 21\n",
    "    \n",
    "    return slope, intercept\n",
    "\n",
    "# least squares using matrix operations\n",
    "def leastSquaresMatrix(X_mat, y):\n",
    "    # check slide 22\n",
    "    \n",
    "    return slope, intercept\n",
    "\n",
    "# time both methods\n",
    "start = time.time()\n",
    "slope_naive, intercept_naive = leastSquaresNaive(X, y)\n",
    "time_naive = time.time() - start\n",
    "\n",
    "X_mat = np.vstack([X, np.ones(n_samples)]).T\n",
    "start = time.time()\n",
    "slope_mat, intercept_mat = leastSquaresMatrix(X_mat, y)\n",
    "time_mat = time.time() - start\n",
    "\n",
    "# print results and time comparison\n",
    "print(\"=== Naive for-loop version ===\")\n",
    "print(f\"Slope: {slope_naive:.4f}, Intercept: {intercept_naive:.4f}, Time: {time_naive*1000:.3f} ms\")\n",
    "\n",
    "print(\"\\n=== Matrix-based version ===\")\n",
    "print(f\"Slope: {slope_mat:.4f}, Intercept: {intercept_mat:.4f}, Time: {time_mat*1000:.3f} ms\")\n",
    "\n",
    "# visualize the fits\n",
    "plt.figure(figsize=(4, 3))\n",
    "\n",
    "plt.scatter(X, y, label='Data', alpha=0.3)\n",
    "plt.plot(X, slope_naive * X + intercept_naive, 'r', label='Loop Fit')\n",
    "plt.plot(X, slope_mat * X + intercept_mat, 'g--', label='Matrix Fit')\n",
    "plt.legend()\n",
    "plt.title(\"Least Squares Fit Comparison\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3733952b",
   "metadata": {},
   "source": [
    "#### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed902bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "X = np.linspace(0, 10, n_samples)\n",
    "true_y = 1 / (1 + np.exp(-3 * (X - 5)))\n",
    "y = true_y + np.random.normal(0, 0.05, size=n_samples)\n",
    "\n",
    "# define model and loss\n",
    "def sigmoid(z):\n",
    "    # check slide 13\n",
    "    \n",
    "    return proba\n",
    "    \n",
    "def predict(X, w, b):\n",
    "    return sigmoid(w * X + b)\n",
    "\n",
    "# gradient descent with automatic stopping\n",
    "def train(X, y, lr=0.1, patience=100, min_delta=1e-5, max_epochs=100000):\n",
    "    w = np.random.randn()\n",
    "    b = np.random.randn()\n",
    "    loss_history = []\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    no_improve_counter = 0\n",
    "\n",
    "    for epoch in tqdm(range(max_epochs), desc=\"Training\"):\n",
    "        # make prediction and compute error\n",
    "        # check slide 24 for computing y_predict and error\n",
    "        \n",
    "        # compute gradients\n",
    "        # check slide 24 for computing grad_w and grad_b\n",
    "        \n",
    "        # early stopping check\n",
    "        loss = np.mean((y_pred - y) ** 2)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        if loss < best_loss - min_delta:\n",
    "            best_loss = loss\n",
    "            no_improve_counter = 0\n",
    "        else:\n",
    "            no_improve_counter += 1\n",
    "\n",
    "        if no_improve_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}. Loss did not improve for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "        # update parameters based on gradients and learning rate\n",
    "        # check slide 25 for updating parameters\n",
    "\n",
    "    return w, b, loss_history\n",
    "\n",
    "# train the model\n",
    "w, b, loss_history = train(X, y, lr=0.1)\n",
    "\n",
    "# plot the result\n",
    "y_fit = predict(X, w, b)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(X, y, \"o\", alpha=0.3, label=\"Noisy data\")\n",
    "plt.plot(X, true_y, \"g--\", label=\"True sigmoid\")\n",
    "plt.plot(X, y_fit, \"r\", label=\"Fitted sigmoid\")\n",
    "plt.title(\"Sigmoid Fit\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ae727",
   "metadata": {},
   "source": [
    "#### Maximum likelyhood estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3450fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = load_breast_cancer()\n",
    "X_raw, y = data.data, data.target  # y âˆˆ {0, 1}\n",
    "feature_names = data.feature_names\n",
    "target_names = data.target_names\n",
    "\n",
    "# print dataset info\n",
    "print(\"Breast Cancer Dataset\")\n",
    "print(f\"Features: {len(feature_names)}\")\n",
    "print(f\"Feature names:\\n{feature_names.tolist()}\")\n",
    "\n",
    "# print a few data samples\n",
    "print(\"\\nFirst 5 samples:\")\n",
    "for i in range(5):\n",
    "    print(f\"X[{i}] = {X_raw[i, :3]}... y = {y[i]}\")  # only show first 3 features\n",
    "\n",
    "# pre-processing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# add bias term (slide 22)\n",
    "X_train_bias = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_test_bias = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "# logistic model functions \n",
    "def sigmoid(z):\n",
    "    # likewise previous exercise\n",
    "\n",
    "def predict(X, w):\n",
    "    return sigmoid(X @ w)\n",
    "\n",
    "def crossEntropy(y_pred, y_true):\n",
    "    # check slide 34\n",
    "    \n",
    "    return ce_loss\n",
    "\n",
    "def accuracy(y_pred_proba, y_true, threshold=0.5):\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    return np.mean(y_pred == y_true)\n",
    "\n",
    "# training function with early stopping\n",
    "def train(X, y, lr=0.1, patience=100, min_delta=1e-5, max_epochs=100000):\n",
    "    w = np.random.randn(X.shape[1])\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    no_improve_counter = 0\n",
    "\n",
    "    for epoch in tqdm(range(max_epochs), desc=\"Training\"):\n",
    "        # make predictions and compute cross entropy loss\n",
    "        # likewise previous exercise\n",
    "        \n",
    "        # compute gradients\n",
    "        # likewise previous exercise\n",
    "\n",
    "        loss_history.append(loss)\n",
    "        acc_history.append(accuracy(y_pred, y))\n",
    "\n",
    "        if loss < best_loss - min_delta:\n",
    "            best_loss = loss\n",
    "            no_improve_counter = 0\n",
    "        else:\n",
    "            no_improve_counter += 1\n",
    "\n",
    "        if no_improve_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}. No improvement for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "        # gradient descent\n",
    "        # likewise previous exercise\n",
    "\n",
    "    return w, loss_history, acc_history\n",
    "\n",
    "# train the model\n",
    "w, loss_history, acc_history = train(X_train_bias, y_train, lr=0.1)\n",
    "\n",
    "# evaluate on test set\n",
    "test_pred = predict(X_test_bias, w)\n",
    "test_acc = accuracy(test_pred, y_test)\n",
    "print(f\"Best train Accuracy: {acc_history[-1] * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# plot loss and accuracy together\n",
    "fig, ax1 = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "ax1.plot(loss_history, color='tab:red', label='Train Loss')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\", color='tab:red')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(acc_history, color='tab:blue', label='Train Accuracy')\n",
    "ax2.set_ylabel(\"Accuracy\", color='tab:blue')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
